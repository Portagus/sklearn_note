{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dislike', 'is', 'life', 'like', 'long', 'py', 'python', 'short', 'too']\n",
      "[[0 1 1 1 0 0 1 1 0]\n",
      " [1 1 1 0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 文本特征化\n",
    "def countvec():\n",
    "    cv = CountVectorizer()\n",
    "    data = cv.fit_transform([\"life is short, i like python\",\n",
    "                             \"life is too long, i dislike py\"])\n",
    "    print(cv.get_feature_names())\n",
    "    # 需要转成数组, 不然还是坐标式\n",
    "    print(data.toarray())\n",
    "    return None\n",
    "# 统计文章中每个词和他们的出现次数\n",
    "if __name__ == \"__main__\":\n",
    "    countvec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本特征抽取 : 文本分类, 情感分析\n",
    "对于单个英文字母 / 汉字 不统计(单个字母不能反映文章的主题)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', '人生', '我不喜欢', '我喜欢', '漫长', '苦短']\n",
      "[[1 1 0 1 0 1]\n",
      " [1 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 文本特征化\n",
    "def countvec():\n",
    "    cv = CountVectorizer()\n",
    "    data = cv.fit_transform([\"人生 苦短, 我喜欢 python\",\n",
    "                             \"人生 漫长, 我不喜欢 python\"])\n",
    "    print(cv.get_feature_names())\n",
    "    # 需要转成数组, 不然还是坐标式\n",
    "    print(data.toarray())\n",
    "    return None\n",
    "# 统计文章中每个词和他们的出现次数\n",
    "if __name__ == \"__main__\":\n",
    "    countvec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天 很 残酷 ,   明天 更 残酷 ,   后天 很 美好 ,   但是 大部分 人死 在 了 明天 晚上 ,   所以 每个 人 都 不要 放弃 我们 看到 的 是从 很远 的 星系 传来 的 光 ,   他们 是 几百万年 前 发出 的 ,   这样 当 我们 看 宇宙 时 ,   我们 只是 在 看 他 的 过去 如果 只是 运用 一种 方式 来 了解 某个 事物 ,   那 就 不会 真正 了解 他 ,   了解 事物 的 真正 含义 取决于 将 其 与 我们 所 了解 的 事物 相连  \n",
      "===============================\n",
      "['一种', '不会', '不要', '了解', '事物', '人死', '今天', '他们', '传来', '但是', '几百万年', '发出', '取决于', '只是', '后天', '含义', '大部分', '如果', '宇宙', '很远', '我们', '所以', '放弃', '方式', '明天', '星系', '是从', '晚上', '某个', '残酷', '每个', '相连', '看到', '真正', '美好', '过去', '运用', '这样']\n",
      "[[0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 0 1 0 2 1 0 0 0 1 0\n",
      "  0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 3 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1\n",
      "  0 1]\n",
      " [1 1 0 4 3 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 2 0 0\n",
      "  1 0]]\n"
     ]
    }
   ],
   "source": [
    "def cutword():\n",
    "    con1 = jieba.cut(\"今天很残酷, 明天更残酷, 后天很美好, 但是大部分人死在了明天晚上, 所以每个人都不要放弃\")\n",
    "    con2 = jieba.cut(\"我们看到的是从很远的星系传来的光, 他们是几百万年前发出的, 这样当我们看宇宙时, 我们只是在看他的过去\")\n",
    "    con3 = jieba.cut(\"如果只是运用一种方式来了解某个事物, 那就不会真正了解他, 了解事物的真正含义取决于将其与我们所了解的事物相连 \")\n",
    "    \n",
    "    # change to list\n",
    "    content1 = list(con1)\n",
    "    content2 = list(con2)\n",
    "    content3 = list(con3)\n",
    "    # 把list change to 字符串\n",
    "    c1 = ' '.join(content1)\n",
    "    c2 = ' '.join(content2)\n",
    "    c3 = ' '.join(content3)\n",
    "    \n",
    "    return c1, c2, c3\n",
    "\n",
    "def hanzivec():\n",
    "    \"\"\"\n",
    "    中文特征值化\n",
    "    return :None\n",
    "    \"\"\"\n",
    "    c1 , c2, c3 = cutword()\n",
    "    print(c1, c2,c3)\n",
    "    cv = CountVectorizer()\n",
    "    data = cv.fit_transform([c1, c2, c3])\n",
    "    print(\"===============================\")\n",
    "    print(cv.get_feature_names())\n",
    "    # 需要转成数组, 不然还是坐标式\n",
    "    print(data.toarray())\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# 统计文章中每个词和他们的出现次数\n",
    "if __name__ == \"__main__\":\n",
    "    hanzivec()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
